{
 "cells": [
  {
   "cell_type": "raw",
   "id": "5932a04e-b75b-43e8-a20d-5e2fb91b55b4",
   "metadata": {},
   "source": [
    "Attention-based Models and Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43de7107-44fa-418a-9a0d-2984f00a3ddd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84f6fb0f-59e9-47e0-9c4c-13a1c99eb6ec",
   "metadata": {},
   "source": [
    "Question 1: What is BERT and how does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e83a93e-6483-4271-85a9-3f896e911dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "62c14384-d69d-4350-aa80-c3ce00f851f6",
   "metadata": {},
   "source": [
    "BERT (Bidirectional Encoder Representations from Transformers) is a deep learning model developed by Google for natural language processing (NLP). It is bidirectional, meaning it considers both previous and next words in a sentence when understanding context. Unlike traditional models, BERT uses the Transformer architecture and self-attention mechanisms to process entire sentences rather than one word at a time.\n",
    "\n",
    "Example\n",
    "Let's say we have the sentence:\n",
    "\"The bank was on the river bank.\"\n",
    "A traditional model might confuse \"bank\" as a financial institution, but BERT understands the context and identifies \"bank\" as the edge of a river."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a5bf7a37-92bc-4465-ab2a-51817a7f8438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.48.2-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m535.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.2)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-macosx_10_12_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-macosx_10_12_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n",
      "Downloading transformers-4.48.2-py3-none-any.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.28.1-py3-none-any.whl (464 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m464.1/464.1 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.2-cp38-abi3-macosx_10_12_x86_64.whl (427 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.1/427.1 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.0-cp39-abi3-macosx_10_12_x86_64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.28.1 safetensors-0.5.2 tokenizers-0.21.0 transformers-4.48.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e13a7ec8-59f3-4f72-b0d7-5f52a29695e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.2.2-cp312-none-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading torch-2.2.2-cp312-none-macosx_10_9_x86_64.whl (150.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.8/150.8 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch\n",
      "Successfully installed torch-2.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1b592afc-48f3-40de-a4a1-13acef25e161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /opt/anaconda3/lib/python3.12/site-packages (2.16.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.32.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.65.4)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.4.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (13.3.5)\n",
      "Requirement already satisfied: namex in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35059571-d8a5-45c9-8935-b7ffbce2c158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.48.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aefc7e1-6d1c-400d-b101-b38ddc028348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a9ee8b444654f4b8889d9dcf173d5aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load pre-trained BERT model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize input text\n",
    "text = \"BERT is a powerful NLP model.\"\n",
    "tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Get BERT embeddings\n",
    "outputs = model(**tokens)\n",
    "print(outputs.last_hidden_state.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90ac133-b196-4779-a776-61d538fbb709",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "7149af89-01a8-4312-a3f0-d0e490997a4f",
   "metadata": {},
   "source": [
    "Question 2: What are the main advantages of using the attention mechanism in neural networks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e282877d-0e0b-491d-8413-b6837aa21206",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eca9415a-e2a5-40c6-a11e-f7885fac550a",
   "metadata": {},
   "source": [
    "The attention mechanism allows neural networks to focus on important parts of input data, improving efficiency and accuracy. It is widely used in transformers, machine translation, text summarization, and image recognition.\n",
    "\n",
    "Better Context Understanding: It helps models capture long-range dependencies in sequences.\n",
    "Parallelization: Unlike RNNs, attention-based models process all words at once, making them faster.\n",
    "Dynamic Focus: The model gives different weights to different words, improving relevance.\n",
    "\n",
    "Example: \n",
    "\n",
    "Attention in a Transformer Model\n",
    "Consider a machine translation task:\n",
    "\"The cat sat on the mat.\" → \"Le chat s'est assis sur le tapis.\"\n",
    "The attention mechanism ensures the model correctly aligns \"cat\" → \"chat\", \"mat\" → \"tapis\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b13edfa-0591-47f9-adf3-5c31c348c80a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc37fe778279410e8c39965491bb3526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.t5.modeling_tf_t5 because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/activations_tf.py:22\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tf_keras'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:1817\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1816\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1818\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/importlib/__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:995\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/t5/modeling_tf_t5.py:30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompiler\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf2xla\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mxla\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dynamic_slice\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations_tf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_tf_activation\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_tf_outputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     32\u001b[0m     TFBaseModelOutput,\n\u001b[1;32m     33\u001b[0m     TFBaseModelOutputWithPastAndCrossAttentions,\n\u001b[1;32m     34\u001b[0m     TFSeq2SeqLMOutput,\n\u001b[1;32m     35\u001b[0m     TFSeq2SeqModelOutput,\n\u001b[1;32m     36\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/activations_tf.py:27\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m parse(keras\u001b[38;5;241m.\u001b[39m__version__)\u001b[38;5;241m.\u001b[39mmajor \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     28\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour currently installed version of Keras is Keras 3, but this is not yet supported in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformers. Please install the backwards-compatible tf-keras package with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install tf-keras`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     31\u001b[0m         )\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_gelu\u001b[39m(x):\n",
      "\u001b[0;31mValueError\u001b[0m: Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load a translation model with attention\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m translator \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranslation_en_to_fr\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt5-small\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Translate sentence\u001b[39;00m\n\u001b[1;32m      7\u001b[0m translation \u001b[38;5;241m=\u001b[39m translator(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe cat sat on the mat.\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/__init__.py:940\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    939\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m--> 940\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m infer_framework_load_model(\n\u001b[1;32m    941\u001b[0m         model,\n\u001b[1;32m    942\u001b[0m         model_classes\u001b[38;5;241m=\u001b[39mmodel_classes,\n\u001b[1;32m    943\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m    944\u001b[0m         framework\u001b[38;5;241m=\u001b[39mframework,\n\u001b[1;32m    945\u001b[0m         task\u001b[38;5;241m=\u001b[39mtask,\n\u001b[1;32m    946\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[1;32m    947\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m    948\u001b[0m     )\n\u001b[1;32m    950\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    951\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/base.py:264\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m         classes\u001b[38;5;241m.\u001b[39mappend(_class)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m look_tf:\n\u001b[0;32m--> 264\u001b[0m     _class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(transformers_module, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTF\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marchitecture\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    266\u001b[0m         classes\u001b[38;5;241m.\u001b[39mappend(_class)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:1806\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1804\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1805\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[0;32m-> 1806\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1807\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n\u001b[1;32m   1808\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:1805\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1803\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[1;32m   1804\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1805\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m   1806\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1807\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:1819\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1818\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1820\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1821\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1822\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.t5.modeling_tf_t5 because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a translation model with attention\n",
    "translator = pipeline(\"translation_en_to_fr\", model=\"t5-small\")\n",
    "\n",
    "# Translate sentence\n",
    "translation = translator(\"The cat sat on the mat.\", max_length=40)\n",
    "print(translation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131b673c-4008-40df-bfd5-488a24f4eda4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "5b5b7e82-c4fd-4183-b896-4601011119f3",
   "metadata": {},
   "source": [
    "Question 3: How does the self-attention mechanism differ from traditional attention mechanisms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca0adbc8-d330-49ad-ba22-4ade9ef56546",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d379c341-b7f5-4825-8d74-fc3fd0ca2a5b",
   "metadata": {},
   "source": [
    "Self-attention is a type of attention mechanism where each word in a sequence attends to all other words to capture relationships.\n",
    "\n",
    "Differences Between Self-Attention and Traditional Attention:\n",
    "Feature\tTraditional Attention\tSelf-Attention\n",
    "Focus\tOne part of input\tEntire input sequence\n",
    "Context\tExternal reference needed\tInternal dependencies learned\n",
    "Example\tMachine translation (encoder-decoder)\tTransformer models (BERT, GPT)\n",
    "\n",
    "Example: \n",
    "Self-Attention Calculation in Transformers\n",
    "Given the sentence \"The dog chased the cat.\", the self-attention mechanism computes relationships like:\n",
    "\n",
    "\"dog\" is related to \"chased.\"\n",
    "\"cat\" is related to \"chased.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9c38843-2f49-4ac8-92dd-cf2086fbac1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6633, 0.3754, 0.4972, 0.3659, 0.3058],\n",
      "        [0.6603, 0.3927, 0.5053, 0.3588, 0.3066],\n",
      "        [0.6167, 0.4167, 0.5836, 0.3211, 0.3517]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Example self-attention scores\n",
    "Q = torch.rand(3, 5)  # Query matrix\n",
    "K = torch.rand(3, 5)  # Key matrix\n",
    "V = torch.rand(3, 5)  # Value matrix\n",
    "\n",
    "# Compute attention scores\n",
    "attention_scores = F.softmax(Q @ K.T / (5 ** 0.5), dim=-1)\n",
    "output = attention_scores @ V\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835be17f-4c70-4580-bc26-5a4104c8af7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "9eb3e78b-9008-451f-9051-ce4e59490be1",
   "metadata": {},
   "source": [
    "Question 4: What is the role of the decoder in a Seq2Seq model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79a0240e-0959-43af-bd4e-a0b6c9c5fa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7597317e-7129-425d-bdd4-c9f7e09785f7",
   "metadata": {},
   "source": [
    "A Seq2Seq (Sequence-to-Sequence) model consists of an encoder (processes input) and a decoder (generates output). The decoder’s job is to convert encoded information into a meaningful sequence, often used in machine translation, chatbots, and summarization.\n",
    "\n",
    "How It Works:\n",
    "Encoder: Converts input into a hidden representation.\n",
    "Decoder: Takes this representation and generates an output step-by-step.\n",
    "Example: English to French Translation\n",
    "Input: \"Hello, how are you?\"\n",
    "Output: \"Bonjour, comment ça va ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de30f770-24e3-4ae0-b7a3-ca4b475f91aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.lstm(x, hidden)\n",
    "        prediction = self.fc(output)\n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b50d15-357d-4374-86d3-4c9c4de579bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "03107121-4c92-47dd-ab6f-a627ec258a99",
   "metadata": {},
   "source": [
    "Question 5: What is the difference between GPT-2 and BERT models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54c8cd31-3d23-4539-b62c-048aacb2bfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e7ef7b80-e534-4b39-970a-5a7e6e810033",
   "metadata": {},
   "source": [
    "Both GPT-2 and BERT are transformer-based models, but they have key differences:\n",
    "\n",
    "Feature                \tBERT\t                                                GPT-2\n",
    "Training\t        Bidirectional\t                                Autoregressive (Left-to-Right)\n",
    "Use Case\t        Understanding texts (NLP tasks)\t                Generating texts (chatbots, writing)\n",
    "Input Processing\tUses entire sentence\t                        Processes word by word\n",
    "Example\t            Text classification, question answering\t        Story completion, chatbot dialogue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4941b9a9-815e-4512-ae30-08782c707f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d01016b3-82ef-4984-b279-d1fb4e66d479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras==2.13.1\n",
      "  Downloading keras-2.13.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: keras\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 3.4.1\n",
      "    Uninstalling keras-3.4.1:\n",
      "      Successfully uninstalled keras-3.4.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.16.2 requires keras>=3.0.0, but you have keras 2.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed keras-2.13.1\n"
     ]
    }
   ],
   "source": [
    "!pip install keras==2.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cfa97fe9-d2d4-423d-94d0-698fca7c1576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.1\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afe01028-8846-4fc5-8cf2-726a51f38e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "INFO: pip is looking at multiple versions of tf-keras to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading tf_keras-2.17.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading tf_keras-2.16.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: tensorflow<2.17,>=2.16 in /opt/anaconda3/lib/python3.12/site-packages (from tf-keras) (2.16.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.32.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.65.4)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.16.2)\n",
      "Collecting keras>=3.0.0 (from tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Downloading keras-3.8.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow<2.17,>=2.16->tf-keras) (0.43.0)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (13.3.5)\n",
      "Requirement already satisfied: namex in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (0.1.0)\n",
      "Downloading tf_keras-2.16.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.8.0-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: keras, tf-keras\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.13.1\n",
      "    Uninstalling keras-2.13.1:\n",
      "      Successfully uninstalled keras-2.13.1\n",
      "Successfully installed keras-3.8.0 tf-keras-2.16.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9391aaac-0bb4-4127-a632-f0069d2b7001",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.4167894423007965, 'token': 3000, 'token_str': 'paris', 'sequence': 'the capital of france is paris.'}, {'score': 0.07141675800085068, 'token': 22479, 'token_str': 'lille', 'sequence': 'the capital of france is lille.'}, {'score': 0.06339266151189804, 'token': 10241, 'token_str': 'lyon', 'sequence': 'the capital of france is lyon.'}, {'score': 0.0444474071264267, 'token': 16766, 'token_str': 'marseille', 'sequence': 'the capital of france is marseille.'}, {'score': 0.030297260731458664, 'token': 7562, 'token_str': 'tours', 'sequence': 'the capital of france is tours.'}]\n"
     ]
    }
   ],
   "source": [
    "#BERT\n",
    "\n",
    "from transformers import pipeline\n",
    "fill_mask = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "print(fill_mask(\"The capital of France is [MASK].\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f3df655-48ca-4ffd-bc3b-da0ec433cd59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c31bcabbf3d443281b731aa70c7119a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4b52d05e2ae42de817d395cc5f70306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14a19ca3a6df49bc9d3de333694b6c86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c09d12c27e0d4db7a3c0c6ebec63f79a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff04a63307bb4d298540cab4bb52e0a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "361f204686cf46b289dbb78993fad176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Once upon a time, this was seen with a bit of trepidation. The only thing keeping him alive was that he was in an old,'}]\n"
     ]
    }
   ],
   "source": [
    "#GPT2\n",
    "\n",
    "from transformers import pipeline\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "print(generator(\"Once upon a time,\", max_length=30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce840ad-fdbf-4e7e-b6cc-672a40639d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "835e0ec6-f135-42de-8a2e-e75b0aa2d218",
   "metadata": {},
   "source": [
    "Question 6: Why is the Transformer model considered more efficient than RNNs and LSTMs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9953c7fb-39da-4c2e-b611-d06033db77d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "019d446e-0ea1-4b3a-84d6-a1bf5b11cb17",
   "metadata": {},
   "source": [
    "The Transformer model is more efficient than RNNs (Recurrent Neural Networks) and LSTMs (Long Short-Term Memory networks) because it processes all words in a sequence at once, instead of one at a time like RNNs.\n",
    "\n",
    "Key Advantages of Transformers Over RNNs & LSTMs:\n",
    "\n",
    "Feature\t                                    RNNs / LSTMs\t                                Transformers\n",
    "Processing\t                        Sequential (one word at a time)\t               Parallel (all words at once)\n",
    "Speed\t                            Slow (due to sequential nature)\t               Fast (uses parallel processing with GPUs/TPUs)\n",
    "Long-Term Dependencies\t            Hard to capture\t                               Easily captures dependencies across entire sequence\n",
    "Scalability\t                        Hard to scale for long texts\t               Scales well with large data\n",
    "Vanishing Gradient\t                Yes, especially in deep networks\t           No vanishing gradient problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d091169-04c4-4eba-9470-4786bf48e95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101, 19081,  2832,  2035,  2616,  1999,  5903,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Sentence to tokenize\n",
    "sentence = \"Transformers process all words in parallel.\"\n",
    "\n",
    "# Tokenize input\n",
    "tokens = tokenizer(sentence, return_tensors=\"pt\")\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cf385a-ab5d-4b18-bfe9-39e29faf653e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "bab7502c-68e5-485f-9924-b12fd9f81a6b",
   "metadata": {},
   "source": [
    "Question 7: Explain how the attention mechanism works in a Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11754d0e-4b88-4a8b-9709-6e0237c1981d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "83d64450-f686-47e1-9910-6bc5a65274ae",
   "metadata": {},
   "source": [
    "The attention mechanism allows the model to focus on relevant words when making predictions. It assigns different weights (importance scores) to words in a sentence.\n",
    "\n",
    "How It Works:\n",
    "\n",
    "Each word in a sentence is compared to all other words.\n",
    "It calculates attention scores using Query (Q), Key (K), and Value (V) matrices.\n",
    "Words that have higher relevance get higher attention scores.\n",
    "\n",
    "Example: Attention in Action\n",
    "Sentence: \"The cat sat on the mat.\"\n",
    "\n",
    "When predicting \"sat\", attention focuses more on \"cat\" than \"mat\".\n",
    "This helps the model understand which words are related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b9157e8-96da-488b-b010-3db59ba720cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5374, 0.5492, 0.3670, 0.6217, 0.2214],\n",
      "        [0.5205, 0.5708, 0.3691, 0.6361, 0.2075],\n",
      "        [0.5130, 0.5548, 0.3514, 0.6180, 0.2142]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Example attention scores\n",
    "Q = torch.rand(3, 5)  # Query matrix\n",
    "K = torch.rand(3, 5)  # Key matrix\n",
    "V = torch.rand(3, 5)  # Value matrix\n",
    "\n",
    "# Compute attention weights\n",
    "attention_scores = F.softmax(Q @ K.T / (5 ** 0.5), dim=-1)\n",
    "output = attention_scores @ V\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9011386b-23ba-4222-91f6-1613037c3d94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "d541aa0a-83af-4b7f-82ff-36bb15ee0e8e",
   "metadata": {},
   "source": [
    "Question 8: What is the difference between an encoder and a decoder in a Seq2Seq model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0803e3dd-6f0b-43b3-a77c-3c3bd89d309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a3f550-eea8-4cba-9ded-538b842a5f3d",
   "metadata": {},
   "source": [
    "A Seq2Seq (Sequence-to-Sequence) model has two main components:\n",
    "\n",
    "Encoder: Processes input and converts it into a hidden representation.\n",
    "Decoder: Uses the encoder’s output to generate the final output sequence.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c4fcaea6-9144-4f53-8213-d6bc873df7bb",
   "metadata": {},
   "source": [
    "Comparison of Encoder vs. Decoder\n",
    "\n",
    "Feature\t                      Encoder\t                                  Decoder\n",
    "Purpose\t                Understand input sentence\t                 Generate output sentence\n",
    "Example Task\t        Read English sentence\t                     Translate to French\n",
    "Architecture            Bi-directional (reads both directions)\t     Auto-regressive (generates one token at a time)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dbdc22ce-5e7d-408c-80c0-d1f1535b65ec",
   "metadata": {},
   "source": [
    "Example: Machine Translation (English → French)\n",
    "Input (Encoder): \"How are you?\"\n",
    "Hidden Representation: Encoded vector\n",
    "Output (Decoder): \"Comment ça va ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "615965b8-bb6c-4066-8882-c0e97aef68e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, hidden = self.lstm(x)\n",
    "        return hidden\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.lstm(x, hidden)\n",
    "        return output, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12743e40-14bc-4e2a-8b8a-5640dcda02e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "1dc6c277-3002-420f-8d41-eb8321d7991f",
   "metadata": {},
   "source": [
    "Question 9: What is the primary purpose of using the self-attention mechanism in transformers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f3236b1-5904-4f5f-be63-e41b1cb6b654",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "134e988a-4687-48b9-8049-7673217cd3de",
   "metadata": {},
   "source": [
    "The self-attention mechanism helps the model focus on relevant words in a sentence, regardless of their position.\n",
    "\n",
    "Why is Self-Attention Important?\n",
    "    \n",
    "Captures long-range dependencies (e.g., understanding context from previous words).\n",
    "Processes entire text at once (unlike RNNs).\n",
    "Improves accuracy in NLP tasks (translation, sentiment analysis, etc.).\n",
    "    \n",
    "Example: Self-Attention in Action\n",
    "Sentence: \"The animal didn't cross the road because it was too tired.\"\n",
    "\n",
    "What does \"it\" refer to? \"animal\" or \"road\"?\n",
    "Self-attention helps the model correctly associate \"it\" with \"animal\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0abfc17f-c930-45af-a78a-af832ebe206b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example input sentence (encoded as tensor)\n",
    "sentence_embedding = torch.rand(5, 10)  # 5 words, 10-dim embeddings\n",
    "\n",
    "# Compute attention weights\n",
    "attention_weights = torch.softmax(torch.matmul(sentence_embedding, sentence_embedding.T), dim=-1)\n",
    "\n",
    "# Apply attention weights\n",
    "output = torch.matmul(attention_weights, sentence_embedding)\n",
    "\n",
    "print(output.shape)  # (5, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4775fc3a-d18c-403b-9992-47cee2342b25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "8cf932d0-05fd-4698-a569-ceb53609299f",
   "metadata": {},
   "source": [
    "Question 10: How does the GPT-2 model generate text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b32d4b2-0912-4195-becb-9a820f6e8e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3784e956-2dce-49c9-8448-4953a60f4ee3",
   "metadata": {},
   "source": [
    "GPT-2 is an autoregressive model, meaning it generates text one word at a time, predicting the next word based on previous ones.\n",
    "\n",
    "How GPT-2 Generates Text:\n",
    "Takes an initial prompt (e.g., \"Once upon a time,\").\n",
    "Predicts the next word (e.g., \"there\").\n",
    "Appends the predicted word to the prompt and repeats the process.\n",
    "Stops when it reaches a limit or detects an end token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e37e9dda-94f5-450a-a6a9-aa0f3ab3612f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, it looks like we're talking about a lot of stuff from the new 'Dance With the Stars's\" song. It\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load GPT-2 model\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "# Generate text based on a prompt\n",
    "prompt = \"Once upon a time,\"\n",
    "output = generator(prompt, max_length=30)\n",
    "\n",
    "print(output[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2564c1-d9ba-42bb-8c6b-03fcb719c581",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "cf058b1f-d33d-4047-9df1-f656a4993443",
   "metadata": {},
   "source": [
    "Question 11: What is the main difference between the encoder-decoder architecture and a simple neural network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3bb4d52-71a0-4c4b-8687-00bd45fbd9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d1625a09-9a1f-4512-8c67-d1e00ac6f247",
   "metadata": {},
   "source": [
    "A simple neural network (feedforward network) takes an input, processes it through layers, and produces an output without sequence dependencies.\n",
    "\n",
    "In contrast, an encoder-decoder architecture is designed for sequence-based tasks like machine translation, text summarization, and speech recognition."
   ]
  },
  {
   "cell_type": "raw",
   "id": "8e1f76f3-aa16-4f01-8a08-3952cec8fe78",
   "metadata": {},
   "source": [
    "Key Differences:\n",
    "\n",
    "Feature\t                        Simple Neural Network\t                    Encoder-Decoder Architecture\n",
    "Input Type\t                      Fixed-length\t                            Variable-length sequences\n",
    "Context Understanding\t          Limited\t                                Captures sequence relationships\n",
    "Examples\t                      Image classification, spam detection\t    Machine translation, chatbot responses\n",
    "Structure\t                      One-step processing\t                    Two-step processing (encode → decode)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ad312a8-a322-4a38-b08c-b8eb4f8ab5ce",
   "metadata": {},
   "source": [
    "Example: Encoder-Decoder for Machine Translation\n",
    "Input (Encoder): \"Hello, how are you?\"\n",
    "Hidden Representation: Encoded vector\n",
    "Output (Decoder): \"Bonjour, comment ça va ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "283a0ff3-b8e5-47ca-abcf-f5b187f465b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, hidden = self.lstm(x)\n",
    "        return hidden\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.lstm(x, hidden)\n",
    "        return output, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b6c002-b81c-4128-a4eb-0749e12c2089",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "7fab984a-5853-4c7b-a451-a9d738f05129",
   "metadata": {},
   "source": [
    "Question 12: Explain the concept of “fine-tuning” in BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a974308a-1451-40d2-944c-1856482cae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9196b567-b8ff-4474-90e6-5b4c815daa54",
   "metadata": {},
   "source": [
    "Fine-tuning in BERT refers to taking a pre-trained BERT model and further training it on a specific task using task-specific data.\n",
    "\n",
    "Why Fine-Tune Instead of Training from Scratch?\n",
    "Saves time and resources (BERT is already trained on vast text data).\n",
    "Learns domain-specific knowledge (e.g., medical, legal, finance texts).\n",
    "Improves accuracy for specific tasks.\n",
    "Example: Fine-Tuning BERT for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6f79f0-e4c3-474c-af69-2de7ceeed0c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "7611ca2c-78be-4e19-a6a2-2f5f426b7a6e",
   "metadata": {},
   "source": [
    "Question 13: How does the attention mechanism handle long-range dependencies in sequences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "94cc6e2c-f585-46d2-a74d-3ed328a831b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a371ce97-9fd6-4cfa-a562-e155f1eeb636",
   "metadata": {},
   "source": [
    "Traditional models like RNNs and LSTMs struggle with long-range dependencies because they process words one at a time, leading to the vanishing gradient problem.\n",
    "\n",
    "The attention mechanism solves this by:\n",
    "\n",
    "Looking at all words at once instead of just recent ones.\n",
    "Assigning weights (importance) to relevant words even if they are far apart.\n",
    "Using self-attention to find relationships across long sentences.\n",
    "Example: Long-Range Dependencies in Attention\n",
    "Sentence: \"The dog that lived in the house near the river chased the cat.\"\n",
    "\n",
    "RNNs/LSTMs might forget \"dog\" when predicting \"chased\".\n",
    "Self-attention correctly links \"dog\" to \"chased\", even with long gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "930c5e33-5f8d-4a5e-bdf1-57b6cde17a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example sentence embeddings\n",
    "sentence_embedding = torch.rand(5, 10)  # 5 words, 10-dimensional embeddings\n",
    "\n",
    "# Compute attention weights\n",
    "attention_weights = torch.softmax(torch.matmul(sentence_embedding, sentence_embedding.T), dim=-1)\n",
    "\n",
    "# Apply attention weights to embeddings\n",
    "output = torch.matmul(attention_weights, sentence_embedding)\n",
    "\n",
    "print(output.shape)  # (5, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702c81ed-1b42-4a24-bba6-144b2546bee2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "81a3fcc2-378a-46cd-a067-1f9ff1aaec5d",
   "metadata": {},
   "source": [
    "Question 14: What is the core principle behind the Transformer architecture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8f747936-4d36-4beb-a3af-5eee7032dd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "84bd2e91-d39e-41e1-9981-c39a184dd867",
   "metadata": {},
   "source": [
    "The Transformer model is based on the idea of self-attention and parallel processing, making it faster and more efficient than RNNs.\n",
    "\n",
    "Key Principles:\n",
    "Self-Attention: Each word interacts with all other words, understanding relationships better.\n",
    "Positional Encoding: Since Transformers don’t use recurrence, they add position information.\n",
    "Feed-Forward Layers: Each attention output goes through a neural network for further learning.\n",
    "Multi-Head Attention: Instead of one attention layer, Transformers use multiple heads to capture different aspects.\n",
    "Example: Transformer Components\n",
    "Encoder: Processes input and creates hidden representations.\n",
    "Decoder: Uses hidden representations to generate output.\n",
    "Self-Attention Mechanism: Helps capture word relationships.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02751af2-447f-4394-a206-97c3421b9510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.ffn = nn.Sequential(nn.Linear(embed_dim, ff_dim), nn.ReLU(), nn.Linear(ff_dim, embed_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        return self.ffn(attn_output)\n",
    "\n",
    "# Example usage\n",
    "transformer = TransformerBlock(embed_dim=64, num_heads=8, ff_dim=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59ada47-d304-4e25-8896-b26d842ba29f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "41e6f654-5e5e-4589-ac96-705f6fe5ffb1",
   "metadata": {},
   "source": [
    "Question 15: What is the role of the \"position encoding\" in a Transformer model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dd27d2fe-c5cf-4df2-9570-c132a5d708fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "853e72b6-b345-47a8-977c-0d2c104c51e4",
   "metadata": {},
   "source": [
    "Unlike RNNs, Transformers do not have a built-in notion of word order. Since Transformers process words in parallel, they need a way to understand the order of words.\n",
    "\n",
    "Positional Encoding helps Transformers learn the order of words by adding unique patterns to each word’s embedding.\n",
    "\n",
    "How It Works\n",
    "Each word’s position is converted into a vector.\n",
    "This vector is added to word embeddings before processing.\n",
    "The model learns positions indirectly while training.\n",
    "Example: Adding Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6908783f-49e0-4a1c-ad6f-bbeebc999613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def positional_encoding(seq_length, embed_dim):\n",
    "    position = np.arange(seq_length)[:, np.newaxis]\n",
    "    div_term = np.exp(np.arange(0, embed_dim, 2) * -(np.log(10000.0) / embed_dim))\n",
    "    pos_encoding = np.zeros((seq_length, embed_dim))\n",
    "    pos_encoding[:, 0::2] = np.sin(position * div_term)\n",
    "    pos_encoding[:, 1::2] = np.cos(position * div_term)\n",
    "    return torch.tensor(pos_encoding, dtype=torch.float32)\n",
    "\n",
    "# Generate positional encodings\n",
    "pos_enc = positional_encoding(seq_length=10, embed_dim=64)\n",
    "print(pos_enc.shape)  # (10, 64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e52bac-8831-4c2e-b247-ea5efd976130",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "ab3e55d3-11b3-46b7-bd2a-f87c8cc0d207",
   "metadata": {},
   "source": [
    "Question 16: How do Transformers use multiple layers of attention?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5efa5eb9-22a9-43b5-accf-d3eb4a0d8af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "24238cd9-8603-454f-b368-b0f4b32224fd",
   "metadata": {},
   "source": [
    "Transformers use multiple layers of self-attention to capture complex relationships between words at different levels. This process is called multi-head attention, where multiple attention heads process different aspects of a sequence in parallel.\n",
    "\n",
    "How It Works:\n",
    "Each word is represented as an embedding (vector).\n",
    "The attention mechanism computes scores to decide which words are important.\n",
    "Instead of using one attention layer, multiple heads capture different meanings or relationships.\n",
    "The outputs from all attention heads are combined and passed through a feed-forward network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b3d10f24-cc86-425b-ba6e-9a4e590c57f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.multihead_attn(x, x, x)\n",
    "        return attn_output\n",
    "\n",
    "# Example usage\n",
    "embed_dim, num_heads = 64, 8\n",
    "attention_layer = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "x = torch.rand(10, 3, 64)  # (sequence_length, batch_size, embedding_dim)\n",
    "output = attention_layer(x)\n",
    "\n",
    "print(output.shape)  # (10, 3, 64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9223f2f-85e6-40da-ba5d-296533734ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "006b53d7-4c1b-4564-ae55-0f9dad087118",
   "metadata": {},
   "source": [
    "Question 17: What does it mean when a model is described as “autoregressive” like GPT-2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "69c8f2ec-fb25-4451-b2c7-c5781229b45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d446eba0-b831-464a-a42a-15748e2baa48",
   "metadata": {},
   "source": [
    "A model is autoregressive when it generates output one step at a time, using previous predictions as input for the next step. This is common in language models like GPT-2, where each word depends on the words before it.\n",
    "\n",
    "How GPT-2 Uses Autoregression\n",
    "Starts with a prompt (e.g., \"The weather today is\").\n",
    "Predicts the next word (e.g., \"sunny\").\n",
    "Uses the predicted word to generate the next word (e.g., \"and\").\n",
    "Repeats the process until reaching a stop condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d65b28d1-544d-4c64-bea3-6106f67b3a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, people thought that he was the greatest president in America and they were wrong.\n",
      "\n",
      "He did, he said, but he\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load GPT-2 model\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "# Generate text based on a prompt\n",
    "prompt = \"Once upon a time,\"\n",
    "output = generator(prompt, max_length=30)\n",
    "\n",
    "print(output[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720a85bb-5b06-4a6d-83a8-015989e2db26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "6810f742-5ff4-4ff3-b2af-df58ade76030",
   "metadata": {},
   "source": [
    "Question 18: How does BERT's bidirectional training improve its performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "74257186-2fa4-4bb2-b75f-b6cf26ec8f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "349ef21b-8b1b-4755-8159-68c019c2e9a4",
   "metadata": {},
   "source": [
    "Unlike traditional models that read text left to right (GPT-2) or right to left, BERT reads in both directions at once. This helps BERT understand full context rather than just previous words.\n",
    "\n",
    "Why Is This Important?\n",
    "Better word understanding: \"bank\" in \"He sat by the bank\" (river) vs \"He went to the bank\" (finance).\n",
    "Improved NLP accuracy: Helps in question answering, text classification, and sentiment analysis.\n",
    "How BERT Learns: Masked Language Modeling (MLM)\n",
    "Randomly masks words in a sentence.\n",
    "Model predicts missing words based on both previous and next words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "60d657f2-c34d-4f12-9df2-e19e3241902b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction 1: the capital of france is paris. (Confidence: 0.4168)\n",
      "Prediction 2: the capital of france is lille. (Confidence: 0.0714)\n",
      "Prediction 3: the capital of france is lyon. (Confidence: 0.0634)\n",
      "Prediction 4: the capital of france is marseille. (Confidence: 0.0444)\n",
      "Prediction 5: the capital of france is tours. (Confidence: 0.0303)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load pre-trained BERT masked language model\n",
    "mask_filler = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "\n",
    "# Input sentence with a masked word\n",
    "sentence = \"The capital of France is [MASK].\"\n",
    "\n",
    "# Predict masked word\n",
    "predictions = mask_filler(sentence)\n",
    "\n",
    "# Display top predictions\n",
    "for idx, pred in enumerate(predictions, start=1):\n",
    "    print(f\"Prediction {idx}: {pred['sequence']} (Confidence: {pred['score']:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b34b08-733a-4404-9137-2dc185e3c6a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "c8a55cc5-3f60-4eb3-8225-c43ea80676ef",
   "metadata": {},
   "source": [
    "Question 19: What are the advantages of using the Transformer over RNN-based models in NLP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "012781e0-943a-40eb-b836-81db8b40698f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "91029171-fc6f-42ee-b72a-0b33c0fa4b9e",
   "metadata": {},
   "source": [
    "Transformers replace RNNs/LSTMs by using self-attention and parallel processing, making them much faster and more efficient.\n",
    "\n",
    "Key Advantages of Transformers Over RNNs/LSTMs\n",
    "\n",
    "Feature\t                                                RNNs / LSTMs\t                    Transformers\n",
    "Processing\t                                    Sequential (one word at a time)\t      Parallel (all words at once)\n",
    "Speed\t                                        Slow\t                              Fast (leverages GPUs/TPUs)\n",
    "Long-Term Dependencies\t                        Hard to capture\t                      Easily captured using self-attention\n",
    "Vanishing Gradient\t                            Yes\t                                  No vanishing gradient problem\n",
    "Scalability\t                                    Difficult for long texts\t          Works well for large datasets\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b028bbcd-ae85-4f2b-b0a2-78bfa71a4ef8",
   "metadata": {},
   "source": [
    "Why are RNNs/LSTMs slower?\n",
    "They process words sequentially, meaning they cannot use parallel computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3e864b0c-0ece-4089-a66d-37f970883198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101, 19081,  2832,  2035,  2616,  1999,  5903,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize input text\n",
    "sentence = \"Transformers process all words in parallel.\"\n",
    "tokens = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6448d5-cdcf-4e31-aee2-0e8ddf3197de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "0125b3bc-f2c4-48fe-a852-54316ce52325",
   "metadata": {},
   "source": [
    "Question 20: What is the attention mechanism’s impact on the performance of models like BERT and GPT-2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e62c5c4d-666a-47c9-a0e4-0e33b1f0dfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9ce1e1b1-8561-456e-80e7-df49be7930d6",
   "metadata": {},
   "source": [
    "The attention mechanism significantly improves performance by allowing models to focus on important words, rather than treating all words equally.\n",
    "\n",
    "How Does Attention Improve BERT and GPT-2?\n",
    "BERT: Uses self-attention to understand word relationships in both directions.\n",
    "GPT-2: Uses causal attention to generate text one word at a time.\n",
    "Improves translation, summarization, and text generation accuracy.\n",
    "Example: Attention in BERT vs. GPT-2\n",
    "BERT: Bidirectional Self-Attention\n",
    "Focuses on all words in the sentence.\n",
    "Learns word meaning based on context from both directions.\n",
    "GPT-2: Causal Self-Attention\n",
    "Looks only at previous words.\n",
    "Generates one word at a time, predicting the most likely next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4abe18a4-bb71-449f-9743-311e9a92251b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example attention scores\n",
    "sentence_embedding = torch.rand(5, 10)  # 5 words, 10-dimensional embeddings\n",
    "\n",
    "# Compute attention weights\n",
    "attention_weights = torch.softmax(torch.matmul(sentence_embedding, sentence_embedding.T), dim=-1)\n",
    "\n",
    "# Apply attention weights to embeddings\n",
    "output = torch.matmul(attention_weights, sentence_embedding)\n",
    "\n",
    "print(output.shape)  # (5, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21120a91-37d1-4b2c-b54f-03acd10e23dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183e302e-5e4a-4ae9-ad65-7f0550ee1d4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "1a453a09-9fdf-4c04-9172-af16001f3c96",
   "metadata": {},
   "source": [
    "Practical Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc143dab-229a-48f8-9dd1-4a7108412c34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "818a49c2-e8bb-4ca9-8fdb-63dcbe0c4220",
   "metadata": {},
   "source": [
    "Question 1: How to implement a simple text classification model using LSTM in Keras?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ae0f7ff7-66f8-41b2-8850-bb14ced5a5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "09720443-08c7-4de2-9870-8ad3269aad43",
   "metadata": {},
   "source": [
    "An LSTM (Long Short-Term Memory) model is effective for text classification because it captures long-term dependencies in text."
   ]
  },
  {
   "cell_type": "raw",
   "id": "1d079b50-b055-4df2-ac06-ac1930b26ec3",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. Preprocess the text (tokenization, padding).\n",
    "2. Use word embeddings (e.g., Embedding layer).\n",
    "3. Apply LSTM layers to process sequences.\n",
    "4. Use a Dense layer for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cb123c35-94fd-4385-ac04-682f243a8564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "labels = np.array(labels)  # Convert labels to NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "43f1a0dc-147e-4504-8bc5-7dce7c9ba3a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Embedding name=embedding_4, built=False>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Embedding(input_dim=5000, output_dim=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f65772fb-a1ee-4b0e-a0f0-20405472817a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 13s/step - accuracy: 0.5000 - loss: 0.6933\n",
      "Epoch 2/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.5000 - loss: 0.6928\n",
      "Epoch 3/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.5000 - loss: 0.6922\n",
      "Epoch 4/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.5000 - loss: 0.6917\n",
      "Epoch 5/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.5000 - loss: 0.6912\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Sample dataset\n",
    "texts = [\"I love this movie!\", \"This is a terrible movie.\", \"Fantastic acting!\", \"Worst film ever.\"]\n",
    "labels = [1, 0, 1, 0]  # 1 = Positive, 0 = Negative\n",
    "\n",
    "# Convert labels to NumPy array\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Tokenize text\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "X = pad_sequences(sequences, maxlen=10)\n",
    "\n",
    "# Define LSTM model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=16),\n",
    "    Bidirectional(LSTM(16, return_sequences=True)),\n",
    "    LSTM(16),\n",
    "    Dense(10, activation=\"relu\"),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# Compile and train\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(X, labels, epochs=5, verbose=1)\n",
    "\n",
    "# Test prediction\n",
    "sample_text = [\"This movie was great!\"]\n",
    "sample_seq = tokenizer.texts_to_sequences(sample_text)\n",
    "sample_seq = pad_sequences(sample_seq, maxlen=10)\n",
    "prediction = model.predict(sample_seq)\n",
    "print(\"Sentiment:\", \"Positive\" if prediction[0] > 0.5 else \"Negative\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ff7aa2-a2f0-4c8f-ad82-8cd7d3baeff4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "63c1864b-d8da-4616-89e8-f6c7ce7fb3b5",
   "metadata": {},
   "source": [
    "Question 2: How to generate sequences of text using a Recurrent Neural Network (RNN)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ce56e6e1-9d57-4200-ade4-316c1ed4f3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0d62d88a-87fa-4741-aede-3136484e46e0",
   "metadata": {},
   "source": [
    "RNNs process sequential text and predict the next word based on previous words.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Tokenize input text.\n",
    "2. Train an RNN model to predict the next word.\n",
    "3. Use the trained model to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ba192b3e-1d95-4b60-848b-25dfbd1e59f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step - accuracy: 0.2500 - loss: 1.7545\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.5000 - loss: 1.7147\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.5000 - loss: 1.6757\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.5000 - loss: 1.6374\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.5000 - loss: 1.5999\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.5000 - loss: 1.5633\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.5000 - loss: 1.5277\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.5000 - loss: 1.4932\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.5000 - loss: 1.4601\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - accuracy: 0.5000 - loss: 1.4286\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 364ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
      "Generated text: hello world world world\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Sample dataset\n",
    "texts = [\"hello world\", \"hello there\", \"hello everyone\", \"hi world\"]\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Prepare sequences\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "X, y = [], []\n",
    "for seq in sequences:\n",
    "    for i in range(1, len(seq)):\n",
    "        X.append(seq[:i])\n",
    "        y.append(seq[i])\n",
    "\n",
    "X = pad_sequences(X, maxlen=5, padding=\"pre\")\n",
    "y = np.array(y)\n",
    "\n",
    "# Define RNN model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=10, input_length=5),\n",
    "    SimpleRNN(16, return_sequences=True),\n",
    "    SimpleRNN(16),\n",
    "    Dense(len(tokenizer.word_index) + 1, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Compile and train\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(X, y, epochs=10, verbose=1)\n",
    "\n",
    "# Generate new text\n",
    "input_text = [\"hello\"]\n",
    "for _ in range(3):  # Generate 3 words\n",
    "    sequence = tokenizer.texts_to_sequences([input_text[-1]])[0]\n",
    "    sequence = pad_sequences([sequence], maxlen=5, padding=\"pre\")\n",
    "    predicted_word_index = np.argmax(model.predict(sequence), axis=-1)\n",
    "    predicted_word = tokenizer.index_word.get(predicted_word_index[0], \"\")\n",
    "    input_text.append(predicted_word)\n",
    "\n",
    "print(\"Generated text:\", \" \".join(input_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce2b0d8-d4c3-404c-be07-39f4a8e5fbac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "014ae6c1-e185-44f7-8b3a-de2a5a7ef857",
   "metadata": {},
   "source": [
    "Question 3: How to perform sentiment analysis using a simple CNN model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "90c86dcb-4dcc-4a33-9671-f58e9f70818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e07b6f29-97eb-4087-9b3e-ee79c91ddaee",
   "metadata": {},
   "source": [
    "A CNN (Convolutional Neural Network) can be used for text classification, where it detects important word patterns.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Convert text into numerical sequences.\n",
    "2. Apply convolutional layers to extract text patterns.\n",
    "3. Use a dense layer for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7accb2a6-00e5-4a36-beba-6a16fb709700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step - accuracy: 0.2500 - loss: 0.6998\n",
      "Epoch 2/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 291ms/step - accuracy: 0.2500 - loss: 0.6956\n",
      "Epoch 3/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - accuracy: 0.7500 - loss: 0.6916\n",
      "Epoch 4/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.7500 - loss: 0.6875\n",
      "Epoch 5/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.7500 - loss: 0.6836\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step\n",
      "Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Sample dataset\n",
    "texts = [\"I love this movie\", \"This movie is terrible\", \"Amazing film!\", \"Worst movie ever\"]\n",
    "labels = [1, 0, 1, 0]  # 1 = Positive, 0 = Negative\n",
    "\n",
    "# Convert labels to NumPy array\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Tokenize and pad text\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "X = pad_sequences(sequences, maxlen=10)\n",
    "\n",
    "# Define CNN model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=16),\n",
    "    Conv1D(32, 3, activation=\"relu\"),\n",
    "    MaxPooling1D(2),\n",
    "    Flatten(),\n",
    "    Dense(10, activation=\"relu\"),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# Compile and train\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(X, labels, epochs=5, verbose=1)\n",
    "\n",
    "# Test prediction\n",
    "sample_text = [\"This movie was awesome!\"]\n",
    "sample_seq = tokenizer.texts_to_sequences(sample_text)\n",
    "sample_seq = pad_sequences(sample_seq, maxlen=10)\n",
    "prediction = model.predict(sample_seq)\n",
    "print(\"Sentiment:\", \"Positive\" if prediction[0] > 0.5 else \"Negative\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9850e412-3207-4eb2-97ef-abf585362ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "8c16f5be-aa27-4777-9f17-87f6b13139ca",
   "metadata": {},
   "source": [
    "Question 4: How to perform Named Entity Recognition (NER) using spaCy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "044320f5-c196-4e50-aa7b-0ecfbe0e95ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d213d198-d208-48c4-bf88-a32ae243af99",
   "metadata": {},
   "source": [
    "Named Entity Recognition (NER) is an NLP technique used to extract names, locations, dates, and other entities from text.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Load a pre-trained spaCy model.\n",
    "2. Apply NER detection on input text.\n",
    "3. Extract and print named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b5a0d23f-91d3-4e94-90e6-7b3a2aba2da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2f591bbc-7f74-49ec-8fe7-c76fa6c33fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.4\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "print(spacy.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ee7c196e-8d7c-4a28-aa4e-0ecdf4c58312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: Apple, Type: ORG\n",
      "Entity: Steve Jobs, Type: PERSON\n",
      "Entity: Cupertino, Type: GPE\n",
      "Entity: California, Type: GPE\n",
      "Entity: 1976, Type: DATE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load pre-trained spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = \"Apple was founded by Steve Jobs in Cupertino, California in 1976.\"\n",
    "\n",
    "# Process text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract named entities\n",
    "for ent in doc.ents:\n",
    "    print(f\"Entity: {ent.text}, Type: {ent.label_}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42afc6e-ae9b-42ed-9030-bdae634c0bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "5aae92a4-b003-4ea5-b7f5-250098a4ebdc",
   "metadata": {},
   "source": [
    "Question 5: How to implement a simple Seq2Seq model for machine translation using LSTM in Keras?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cbfd64af-35d7-4b12-aa11-cacd7b953f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f9bbed7c-a7b1-4751-a932-c6bf6d82fbc7",
   "metadata": {},
   "source": [
    "A Seq2Seq (Sequence-to-Sequence) model is used in machine translation (e.g., English → French). It consists of:\n",
    "\n",
    "Encoder: Converts input sentences into a fixed-length vector.\n",
    "Decoder: Translates the encoded vector into the target language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "941fcf1d-2727-4c38-8568-ee5fb098b278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">640,000</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">640,000</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │                   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), │    <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],       │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]        │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,285,000</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m640,000\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m640,000\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)         │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),     │    \u001b[38;5;34m394,240\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │                   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m256\u001b[0m), │    \u001b[38;5;34m394,240\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],       │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]        │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m5000\u001b[0m)  │  \u001b[38;5;34m1,285,000\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,353,480</span> (12.79 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,353,480\u001b[0m (12.79 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,353,480</span> (12.79 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,353,480\u001b[0m (12.79 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, Input\n",
    "\n",
    "# Define vocabulary size and sequence length\n",
    "input_vocab_size = 5000\n",
    "output_vocab_size = 5000\n",
    "sequence_length = 10\n",
    "embedding_dim = 128\n",
    "hidden_units = 256\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(sequence_length,))\n",
    "encoder_embedding = Embedding(input_vocab_size, embedding_dim)(encoder_inputs)\n",
    "encoder_lstm = LSTM(hidden_units, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(sequence_length,))\n",
    "decoder_embedding = Embedding(output_vocab_size, embedding_dim)(decoder_inputs)\n",
    "decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=[state_h, state_c])\n",
    "decoder_dense = Dense(output_vocab_size, activation=\"softmax\")\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b67770-4163-44d2-a6d9-41c8b6ab5ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "b64f7364-a070-48f1-bab7-5ceef87e481b",
   "metadata": {},
   "source": [
    "Question 6: How to generate text using a pre-trained transformer model (GPT-2)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f6ac47a3-be6e-4f9e-937c-ab6c7593e8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "13e130dc-9c29-422d-8ca5-90d9b51d3d40",
   "metadata": {},
   "source": [
    "GPT-2 is a transformer-based text generation model that predicts the next word based on previous words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36a6f67c-0ad6-4aae-a0bc-1842036dcbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, when our lives were the same and not the same, there was one single night and my husband was with me. Our daughter came over to him and he said, \"Dad will never think you're bad again. She'll\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load GPT-2 model\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "# Generate text based on a prompt\n",
    "prompt = \"Once upon a time,\"\n",
    "output = generator(prompt, max_length=50)\n",
    "\n",
    "# Print generated text\n",
    "print(output[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244b6dc7-66e4-479b-91c4-72aaa58f3e81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "836719e8-0b81-40aa-b978-ccff69284b1f",
   "metadata": {},
   "source": [
    "Question 7: How to apply data augmentation for text in NLP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "50cb382c-b79b-4fa5-8584-5b7787b100da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8b63aa32-f257-45cf-ba03-bfe878d33615",
   "metadata": {},
   "source": [
    "Data augmentation helps improve model performance by creating variations of training data.\n",
    "\n",
    "Common NLP Data Augmentation Techniques\n",
    "\n",
    "1. Synonym Replacement (Replace words with synonyms)\n",
    "2. Random Insertion (Insert random words)\n",
    "3. Random Deletion (Remove words)\n",
    "4. Back Translation (Translate to another language, then back to the original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fd5b13b-d590-4048-a136-18e25b3a67d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nlpaug\n",
      "  Downloading nlpaug-1.1.11-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.16.2 in /opt/anaconda3/lib/python3.12/site-packages (from nlpaug) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from nlpaug) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from nlpaug) (2.32.2)\n",
      "Collecting gdown>=4.0.0 (from nlpaug)\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.12/site-packages (from gdown>=4.0.0->nlpaug) (4.12.3)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from gdown>=4.0.0->nlpaug) (3.13.1)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from gdown>=4.0.0->nlpaug) (4.66.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.2.0->nlpaug) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.2.0->nlpaug) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.2.0->nlpaug) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.22.0->nlpaug) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.22.0->nlpaug) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.22.0->nlpaug) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.22.0->nlpaug) (2025.1.31)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.5)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/anaconda3/lib/python3.12/site-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\n",
      "Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: gdown, nlpaug\n",
      "Successfully installed gdown-5.2.0 nlpaug-1.1.11\n"
     ]
    }
   ],
   "source": [
    "!pip install nlpaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "208b7126-f03d-44cd-9c02-8df70a80c1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/mdrizwanalam/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/mdrizwanalam/nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/mdrizwanalam/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: The quick brown fox jumps over the lazy dog.\n",
      "Augmented: ['The speedy brown fox jumps over the faineant andiron.']\n"
     ]
    }
   ],
   "source": [
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "# Define text\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Synonym Augmentation\n",
    "aug = naw.SynonymAug()\n",
    "augmented_text = aug.augment(text)\n",
    "\n",
    "# Print augmented text\n",
    "print(\"Original:\", text)\n",
    "print(\"Augmented:\", augmented_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721e09e4-1555-46ab-9091-8e0be411117d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "06440b57-6257-46e8-bf11-6835b4f33efe",
   "metadata": {},
   "source": [
    "Question 8: How can you add an Attention Mechanism to a Seq2Seq model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cda2f6dc-635c-4faa-8930-5ae36643b678",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "80bdfa21-4809-419d-8617-0cfd3140e713",
   "metadata": {},
   "source": [
    "Attention allows the decoder to focus on different parts of the input sequence instead of just relying on the last hidden state.\n",
    "\n",
    "How Attention Helps\n",
    "Helps models handle long sentences.\n",
    "Improves performance in translation, summarization, and chatbots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2065e6e-9964-48e7-b324-b7d5c084471a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received: input_shape=[(None, 1, 256), (None, 10, 256)]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m context_vector, _ \u001b[38;5;241m=\u001b[39m attention_layer(encoder_outputs, state_h)  \u001b[38;5;66;03m# Use state_h directly\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Concatenate context vector with decoder output\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m decoder_concat_input \u001b[38;5;241m=\u001b[39m Concatenate(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)([context_vector, decoder_outputs])\n\u001b[1;32m     42\u001b[0m decoder_dense \u001b[38;5;241m=\u001b[39m Dense(\u001b[38;5;241m5000\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m)(decoder_concat_input)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Define Model\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/merging/concatenate.py:99\u001b[0m, in \u001b[0;36mConcatenate.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m     93\u001b[0m         unique_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\n\u001b[1;32m     94\u001b[0m             shape[axis]\n\u001b[1;32m     95\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m shape \u001b[38;5;129;01min\u001b[39;00m shape_set\n\u001b[1;32m     96\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m shape[axis] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     97\u001b[0m         )\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_dims) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 99\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err_msg)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received: input_shape=[(None, 1, 256), (None, 10, 256)]"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, LSTM, Embedding, Dense, Input, Concatenate\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Define Attention Layer\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "\n",
    "    def call(self, encoder_outputs, decoder_hidden_state):\n",
    "        # Expand decoder state for batch matrix multiplication\n",
    "        decoder_hidden_expanded = K.expand_dims(decoder_hidden_state, axis=1)\n",
    "\n",
    "        # Compute attention scores using batch_dot\n",
    "        attention_scores = K.batch_dot(decoder_hidden_expanded, encoder_outputs, axes=[2, 2])\n",
    "\n",
    "        # Compute attention weights (softmax)\n",
    "        attention_weights = tf.nn.softmax(attention_scores, axis=1)\n",
    "\n",
    "        # Compute context vector\n",
    "        context_vector = K.batch_dot(attention_weights, encoder_outputs, axes=[2, 1])\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "# Define Encoder\n",
    "encoder_inputs = Input(shape=(10,))\n",
    "encoder_embedding = Embedding(5000, 128)(encoder_inputs)\n",
    "encoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "\n",
    "# Define Decoder\n",
    "decoder_inputs = Input(shape=(10,))\n",
    "decoder_embedding = Embedding(5000, 128)(decoder_inputs)\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=[state_h, state_c])\n",
    "\n",
    "# Apply Attention\n",
    "attention_layer = AttentionLayer()\n",
    "context_vector, _ = attention_layer(encoder_outputs, state_h)  # Use state_h directly\n",
    "\n",
    "# Concatenate context vector with decoder output\n",
    "decoder_concat_input = Concatenate(axis=-1)([context_vector, decoder_outputs])\n",
    "decoder_dense = Dense(5000, activation=\"softmax\")(decoder_concat_input)\n",
    "\n",
    "# Define Model\n",
    "model = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_dense)\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185bf2de-e2fc-4a57-b5c8-deb1393b7a55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce020a1-2fc7-45c8-bbb0-c2f0582d19c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
